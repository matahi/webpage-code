<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
	<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
			(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-53419684-1', 'auto');
	ga('send', 'pageview');

</script>


	<head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <!--
  <title>
    
      bio-info, SVM and Graph-kernels &middot; 
    
  </title> -->
  <title>Matahi</title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-144-precomposed.png">
                                 <link rel="shortcut icon" href="/public/favicon.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">
</head>


	<body class="theme-base-08">

		<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1></h1>
      <p class="lead"></p>
    </div>

    <ul class="sidebar-nav">
      <li class="sidebar-nav-item">
        <a href="/">Home</a>
      </li>

      

      
      
        
          
        
      
        
      
        
          
        
      
        
          
            <li class="sidebar-nav-item">
              <a href="/projects/index.html">Projects</a>
            </li>
          
        
      
        
          
            <li class="sidebar-nav-item">
              <a href="/publications/index.html">Publications</a>
            </li>
          
        
      
        
          
            <li class="sidebar-nav-item">
              <a href="/resume/index.html">Resume</a>
            </li>
          
        
      
        
          
            <li class="sidebar-nav-item">
              <a href="/teaching/index.html">Teaching</a>
            </li>
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
    </ul>

    <!-- <p>&copy; 2015. All rights reserved.</p> -->
  </div>
</div>


		<div class="content container">
			<div class="post">
  <h1 class="post-title">bio-info, SVM and Graph-kernels</h1>
  <span class="post-date">22 May 2015</span>
  <h1>Introduction</h1>

<p>This page is the practical session of the &quot;Support Vector Machines&quot; module taught by <a href="http://cazencott.info">Chlo√©-Agathe Azencott</a>.</p>

<h1>Material</h1>

<p>The practical session is done using <a href="http://cran.r-project.org">R</a>. For a quick tutorial, follow this <a href="http://pages.pomona.edu/%7Ejsh04747/courses/R.pdf">link</a>.</p>

<h1>Practical Session</h1>

<h2>Necessary support:</h2>

<ul>
<li>The lecture <a href="http://cazencott.info/dotclear/public/lectures/2015-S1234-svm.pdf">slides</a></li>
<li>The practical session R and the datasets <a href="http://matahi.github.io/downloads/TPMines2015.zip">file</a> </li>
</ul>

<h3>Part A: Linear dataset.</h3>

<p><strong>A.1/A.2)</strong> Visualization of the first dataset:</p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="kp">load</span><span class="p">(</span><span class="s">&#39;linear1.RData&#39;</span><span class="p">)</span>
</code></pre></div>
<p>3 new variables are in the environment now (type <code>ls()</code> to check):</p>

<ul>
<li>linear1.train : a matrix of size n1x3 (first column is <em>x1</em>, second column is <em>x2</em> and third column is the output <em>y</em>). </li>
<li>linear1.test.input : a matrix of size n2 x 2 (same as the train but without the output <em>y</em>).</li>
<li>linear1.data : the combined dataset (for visualization).</li>
</ul>

<p>Then visualize the dataset:</p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="kn">require</span><span class="p">(</span> <span class="s">&#39;ggplot2&#39;</span> <span class="p">)</span>
qplot<span class="p">(</span> data<span class="o">=</span>linear1.data<span class="p">,</span> x.1<span class="p">,</span> x.2<span class="p">,</span> colour<span class="o">=</span><span class="kp">factor</span><span class="p">(</span>y<span class="p">),</span> 
                    shape<span class="o">=</span><span class="kp">factor</span><span class="p">(</span>train<span class="p">)</span> <span class="p">)</span>
</code></pre></div>
<p><strong>Question 1:</strong> How can you characterise this dataset ?</p>

<p>Given what we learned during the lecture, we train a <strong>linear SVM</strong> on the training set which will be our predictor for the testing set.</p>

<p><strong>A.3/A.4/A.5)</strong> Training the SVM:</p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="c1">### A.3) Train a linear SVM</span>
<span class="kn">require</span><span class="p">(</span> <span class="s">&#39;kernlab&#39;</span> <span class="p">)</span>
linear1.svm <span class="o">&lt;-</span> ksvm<span class="p">(</span> y <span class="o">~</span> <span class="m">.</span><span class="p">,</span> data<span class="o">=</span>linear1.train<span class="p">,</span> type<span class="o">=</span><span class="s">&#39;C-svc&#39;</span><span class="p">,</span> 
                        kernel<span class="o">=</span><span class="s">&#39;vanilladot&#39;</span><span class="p">,</span>
                        C<span class="o">=</span><span class="m">100</span><span class="p">,</span> scale<span class="o">=</span><span class="kt">c</span><span class="p">()</span> <span class="p">)</span>

<span class="c1">### A.4) Plot the model</span>
plot<span class="p">(</span> linear1.svm<span class="p">,</span> data<span class="o">=</span>linear1.train <span class="p">)</span>

<span class="c1">### A.5) Adding points of test on the graph</span>
points<span class="p">(</span> linear1.test.input<span class="p">[</span> <span class="kp">sample.int</span><span class="p">(</span><span class="kp">nrow</span><span class="p">(</span>linear1.test.input<span class="p">),</span><span class="m">10</span><span class="p">),</span> <span class="p">],</span> pch<span class="o">=</span><span class="m">4</span> <span class="p">)</span>
</code></pre></div>
<p><strong>Question 2:</strong> What are the black points in the figure ?</p>

<p><strong>A.6/A.7</strong> Testing on another dataset:</p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="c1">### A.6) Prediction</span>
linear1.prediction <span class="o">&lt;-</span> predict<span class="p">(</span> linear1.svm<span class="p">,</span> linear1.test.input <span class="p">)</span>

<span class="c1">### A.7) Look at accuracy</span>
<span class="kp">load</span><span class="p">(</span><span class="s">&#39;linear1Sol.RData&#39;</span><span class="p">)</span>
<span class="c1"># contains linear1.test.output</span>

<span class="kp">print</span><span class="p">(</span><span class="kp">paste0</span><span class="p">(</span><span class="s">&#39;Accuracy: &#39;</span><span class="p">,</span> <span class="kp">floor</span><span class="p">(</span><span class="m">100</span><span class="o">*</span><span class="kp">sum</span><span class="p">(</span> linear1.prediction <span class="o">==</span> linear1.test.output <span class="p">)</span><span class="o">/</span><span class="kp">length</span><span class="p">(</span>linear1.test.output<span class="p">)),</span> <span class="s">&#39;%&#39;</span><span class="p">))</span>
</code></pre></div>
<p>Let&#39;s consider a dataset a bit more complex:</p>

<p><strong>A.9 to A.14)</strong> This is the exact same thing as the first part except that the dataset is non-separable.</p>

<p><strong>Question 3:</strong> Is the accuracy a sufficient method to assess the performance of a model?</p>

<p>Let&#39;s discuss a few ways to improve the assessment of the performance:</p>

<p><strong>A.15)</strong> Separate positive and negative examples :</p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="c1">### A.15) A confusion matrix gives more information than just accuracy</span>
<span class="kp">print</span><span class="p">(</span><span class="s">&#39;Confusion Matrix: &#39;</span><span class="p">);</span><span class="kp">print</span><span class="p">(</span><span class="kp">table</span><span class="p">(</span> linear2.prediction<span class="p">,</span> linear2.test.output<span class="p">,</span> dnn<span class="o">=</span> <span class="kt">c</span><span class="p">(</span><span class="s">&quot;prediction&quot;</span><span class="p">,</span><span class="s">&quot;reality&quot;</span><span class="p">)</span> <span class="p">))</span>
</code></pre></div>
<p><strong>A.16)</strong> ROC Curves (See <a href="http://en.wikipedia.org/wiki/Receiver_operating_characteristic">wikipedia</a>):</p>
<div class="highlight"><pre><code class="language-r" data-lang="r">linear2.prediction.score <span class="o">&lt;-</span> predict<span class="p">(</span> linear2.svm<span class="p">,</span> linear2.test.input<span class="p">,</span> type<span class="o">=</span><span class="s">&#39;decision&#39;</span> <span class="p">)</span>

<span class="kn">require</span><span class="p">(</span> <span class="s">&#39;ROCR&#39;</span> <span class="p">)</span>

<span class="c1">## ROC</span>
linear2.roc.curve <span class="o">&lt;-</span> performance<span class="p">(</span> prediction<span class="p">(</span> linear2.prediction.score<span class="p">,</span> linear2.test.output <span class="p">),</span> measure<span class="o">=</span><span class="s">&#39;tpr&#39;</span><span class="p">,</span> x.measure<span class="o">=</span><span class="s">&#39;fpr&#39;</span> <span class="p">)</span>
plot<span class="p">(</span> linear2.roc.curve <span class="p">)</span>
</code></pre></div>
<h3>Part B: Non-Linear dataset.</h3>

<p><strong>B.1/B.2/B.3)</strong> Trying linear SVM on a dataset where it is not appropriate.</p>

<p>Let&#39;s try a different kernel</p>

<p><strong>B.4)</strong> RBF Kernel:</p>
<div class="highlight"><pre><code class="language-r" data-lang="r">nonlinear.svm <span class="o">&lt;-</span> ksvm<span class="p">(</span> y <span class="o">~</span> <span class="m">.</span><span class="p">,</span> data<span class="o">=</span>nonlinear.train<span class="p">,</span> type<span class="o">=</span><span class="s">&#39;C-svc&#39;</span><span class="p">,</span> 
                  kernel<span class="o">=</span><span class="s">&#39;rbf&#39;</span><span class="p">,</span> kpar<span class="o">=</span><span class="kt">list</span><span class="p">(</span>sigma<span class="o">=</span><span class="m">1</span><span class="p">),</span> 
                  C<span class="o">=</span><span class="m">100</span><span class="p">,</span> scale<span class="o">=</span><span class="kt">c</span><span class="p">()</span> <span class="p">)</span>
plot<span class="p">(</span> nonlinear.svm<span class="p">,</span> data<span class="o">=</span>nonlinear.train <span class="p">)</span>
</code></pre></div>
<p><strong>Question 4:</strong> Recall what is the parameter <code>C</code> in the svm. In the following we will see what happens when we vary <code>C</code>.</p>

<p><strong>B.5)</strong> Impact of <code>C</code></p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="kn">require</span><span class="p">(</span><span class="s">&#39;manipulate&#39;</span><span class="p">)</span>
manipulate<span class="p">(</span> plot<span class="p">(</span> ksvm<span class="p">(</span> y <span class="o">~</span> <span class="m">.</span><span class="p">,</span> data<span class="o">=</span>nonlinear.train<span class="p">,</span> type<span class="o">=</span><span class="s">&#39;C-svc&#39;</span><span class="p">,</span> 
            kernel<span class="o">=</span>k<span class="p">,</span> C<span class="o">=</span><span class="m">2</span><span class="o">^</span>c.exponent<span class="p">,</span> scale<span class="o">=</span><span class="kt">c</span><span class="p">()</span> <span class="p">),</span>
            data<span class="o">=</span>nonlinear.train <span class="p">),</span> 
           c.exponent<span class="o">=</span>slider<span class="p">(</span><span class="m">-10</span><span class="p">,</span><span class="m">10</span><span class="p">),</span>
                   k<span class="o">=</span>picker<span class="p">(</span><span class="s">&#39;Gaussian&#39;</span><span class="o">=</span><span class="s">&#39;rbfdot&#39;</span><span class="p">,</span> <span class="s">&#39;Linear&#39;</span><span class="o">=</span><span class="s">&#39;vanilladot&#39;</span><span class="p">,</span> 
                    <span class="s">&#39;Hyperbolic&#39;</span><span class="o">=</span><span class="s">&#39;tanhdot&#39;</span><span class="p">,</span><span class="s">&#39;Spline&#39;</span><span class="o">=</span><span class="s">&#39;splinedot&#39;</span><span class="p">,</span> 
                <span class="s">&#39;Laplacian&#39;</span><span class="o">=</span><span class="s">&#39;laplacedot&#39;</span><span class="p">)</span> <span class="p">)</span>
</code></pre></div>
<p><strong>B.6)</strong> Visualization of the impact of <code>C</code> on the prediction accuracy (<strong>Bias-Variance Tradeoff</strong>):</p>
<div class="highlight"><pre><code class="language-r" data-lang="r"><span class="c1">### B.6) Bias-Variance Tradeoff</span>
BiasVarianceTradeoff <span class="o">&lt;-</span> <span class="kr">function</span><span class="p">(</span> dataset<span class="p">,</span> cross<span class="o">=</span><span class="m">10</span><span class="p">,</span> c.seq<span class="o">=</span><span class="m">2</span><span class="o">^</span><span class="kp">seq</span><span class="p">(</span><span class="m">-10</span><span class="p">,</span> <span class="m">10</span><span class="p">),</span> <span class="kc">...</span> <span class="p">)</span> <span class="p">{</span>
  err <span class="o">&lt;-</span> <span class="kp">sapply</span><span class="p">(</span> c.seq<span class="p">,</span> <span class="kr">function</span><span class="p">(</span> <span class="kt">c</span> <span class="p">)</span> 
                <span class="p">{</span>
                        cross<span class="p">(</span> ksvm<span class="p">(</span> y <span class="o">~</span> <span class="m">.</span><span class="p">,</span> data<span class="o">=</span>dataset<span class="p">,</span> C<span class="o">=</span><span class="kt">c</span><span class="p">,</span> cross<span class="o">=</span>cross<span class="p">,</span> <span class="kc">...</span><span class="p">)</span> <span class="p">)</span>
                <span class="p">})</span>
  <span class="kr">return</span><span class="p">(</span><span class="kt">data.frame</span><span class="p">(</span> <span class="kt">c</span><span class="o">=</span>c.seq<span class="p">,</span> error<span class="o">=</span>err <span class="p">))</span>
<span class="p">}</span>

qplot<span class="p">(</span> <span class="kt">c</span><span class="p">,</span> error<span class="p">,</span> data<span class="o">=</span>BiasVarianceTradeoff<span class="p">(</span> nonlinear.train<span class="p">,</span> type<span class="o">=</span><span class="s">&#39;C-svc&#39;</span><span class="p">,</span> kernel<span class="o">=</span><span class="s">&#39;rbfdot&#39;</span> <span class="p">),</span> geom<span class="o">=</span><span class="s">&#39;line&#39;</span><span class="p">,</span> log<span class="o">=</span><span class="s">&#39;x&#39;</span> <span class="p">)</span>
</code></pre></div>
<p><strong>Question 5:</strong> How to choose <code>C</code> ?</p>

<h3>Part C: Acute lymphoblastic leukemia dataset.</h3>

<p>In this part, we work on a real public dataset of <em>Acute lymphoblastic leukemia (ALL)</em> patients.</p>

<p><strong>C.1)</strong> We are interested in classifying leukemia patients into two classes: <em>B-cell ALL</em> vs <em>T-cell ALL</em>  because this classification can have an impact on the <strong>patient&#39;s prognosis</strong> or <strong>its response to a given treatment</strong>.</p>

<p><strong>C.2)</strong> When we add supplementary clinical information, we have more than 2 classes (<em>B1</em>, <em>B2</em>,...).  </p>

<p><strong>Question 6:</strong> How can you derive from what you learned a SVM classifier that can predict more than 2 classes?</p>

</div>



		</div>

	</body>
</html>
